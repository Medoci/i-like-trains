{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d5f0af5",
   "metadata": {},
   "source": [
    "# Data Science Case Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb05be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "\n",
    "import holidays\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "pd.options.display.max_columns = 500\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317c41fc",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075246fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path.cwd().resolve().parents[1] / \"data\"\n",
    "sales = pd.read_csv(data_path / \"sales.csv\", index_col=0)\n",
    "stations = pd.read_csv(data_path / \"stations.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8296481",
   "metadata": {},
   "source": [
    "___Sales Data Dictionary___\n",
    "\n",
    "    - date: yyyy-mm-dd format. increments of one day. not unique.\n",
    "    - sales: number of sales on that given day, relative to the station\n",
    "    - station: name of a station in the UK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76aee1e",
   "metadata": {},
   "source": [
    "___Stations Data Dictionary___\n",
    "\n",
    "    - station: name of the station in the UK\n",
    "    - lat: latitude coordinate\n",
    "    - lon: longitude coordinate\n",
    "    - operator: responible entity for the ticket / journey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ab372c",
   "metadata": {},
   "source": [
    "## Operator Observation\n",
    "\n",
    "    Trainline is a different kind of entity to English / Welsh / Scottish Rail. \n",
    "    The latter is a private or public body that own the trains and journeys\n",
    "    the other is a vendor which sells tickets on behalf of these bodies.\n",
    "\n",
    "***Why this matters***\n",
    "\n",
    "    Railways may sell tickets through many channels and Trainline is simply just one of them (i.e. tickets at station, their own website etc). A sale to trainline is also a sale to whichever railway operates at the station.\n",
    "\n",
    "    We will need to map stations to the correct body based on the lat/lon data so that sales totals for each railway is fully represenative. \n",
    "    (e.g. when lat/lon falls inside the Wales then attribute sales to Welsh Railway.)\n",
    "\n",
    "***Risks***\n",
    "\n",
    "    Trainline take a portion of the ticket sales as revenue and the rest is passed onto the body that owns the station. Care will need to be taken to ensure reasonable estimates.\n",
    "\n",
    "\n",
    "***From now on***\n",
    "\n",
    "***Railways - refers to the english/welsh/scottish railway companies***\n",
    "\n",
    "***Trainline - will be refered to by its namesake***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809fc244",
   "metadata": {},
   "source": [
    "## Engineering Features & Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6efcd0b",
   "metadata": {},
   "source": [
    "### Mapping stations to nations\n",
    "\n",
    "Downloaded GeoJSON object from Gov Website for mapping\n",
    "https://geoportal.statistics.gov.uk/datasets/e4cab2a2419f46d7847dafb50a159aa7_0/explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00075288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From my understanding given the resolution of the gov file, error could be up to 1000m. \n",
    "# This is forgivable given that nations are large. \n",
    "# If error is over 1000m then it is likely mapped poorly\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "gdf_stations = gpd.GeoDataFrame(\n",
    "    stations,\n",
    "    geometry=gpd.points_from_xy(stations[\"lon\"], stations[\"lat\"]),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "countries = gpd.read_file(data_path / \"countries.geojson\").to_crs(gdf_stations.crs)\n",
    "\n",
    "# Buffer stations by x meters then check point-in-polygon\n",
    "gdf_stations_m = gdf_stations.to_crs(27700)\n",
    "countries_m = countries.to_crs(27700)\n",
    "x = 1000\n",
    "gdf_stations_buf = gdf_stations_m.copy()\n",
    "gdf_stations_buf[\"geometry\"] = gdf_stations_buf.buffer(x)\n",
    "\n",
    "joined = gpd.sjoin(\n",
    "    gdf_stations_buf,\n",
    "    countries_m[[\"CTRY22NM\", \"geometry\"]],\n",
    "    how=\"left\",\n",
    "    predicate=\"intersects\"\n",
    ")\n",
    "gdf_stations_w_nation = (\n",
    "    joined.rename(columns={\"CTRY22NM\": \"nation\"})\n",
    "          .drop(columns=[\"geometry\", \"index_right\"], errors=\"ignore\")\n",
    ")\n",
    "\n",
    "# Checking errors clear\n",
    "gdf_stations_w_nation.query(\"nation.isna()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4ff621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check our coordinates - none should have errors above 1000m\n",
    "station = \"Westcliff\"\n",
    "nation = \"England\"\n",
    "\n",
    "def _pick_station_row(df: gpd.GeoDataFrame, station: str, prefer_non_null_nation=True):\n",
    "    sub = df.loc[df[\"station\"] == station]\n",
    "    if sub.empty:\n",
    "        raise ValueError(f\"Station '{station}' not found.\")\n",
    "    if prefer_non_null_nation and \"nation\" in sub.columns and sub[\"nation\"].notna().any():\n",
    "        sub = sub[sub[\"nation\"].notna()]\n",
    "    return sub.iloc[0]\n",
    "\n",
    "def zoom_station(df: gpd.GeoDataFrame, station: str, pad_deg: float = 1.0):\n",
    "    row = _pick_station_row(df, station)\n",
    "    pt = gpd.GeoSeries([Point(row.lon, row.lat)], crs=\"EPSG:4326\")\n",
    "\n",
    "    ax = countries.plot(figsize=(6, 6), edgecolor=\"purple\", facecolor=\"none\")\n",
    "    pt.plot(ax=ax, markersize=80)\n",
    "    plt.xlim(row.lon - pad_deg, row.lon + pad_deg)\n",
    "    plt.ylim(row.lat - pad_deg, row.lat + pad_deg)\n",
    "    title_nation = row[\"nation\"] if \"nation\" in row else None\n",
    "    plt.title(f\"{station} (nation={title_nation})\")\n",
    "    plt.show()\n",
    "\n",
    "def location_offset_error(station: str, nation: str) -> float:\n",
    "    row = _pick_station_row(gdf_stations, station, prefer_non_null_nation=False)\n",
    "    pt = gpd.GeoSeries([Point(row.lon, row.lat)], crs=\"EPSG:4326\").to_crs(27700).iloc[0]\n",
    "    sel = countries.loc[countries[\"CTRY22NM\"] == nation]\n",
    "    if sel.empty:\n",
    "        raise ValueError(f\"Nation '{nation}' not found in countries['CTRY22NM'].\")\n",
    "    poly = sel.to_crs(27700).geometry.union_all()\n",
    "    dist_m = pt.distance(poly)\n",
    "    print(f\"{station} â†’ {nation}: {dist_m:.1f} m\")\n",
    "    return float(dist_m)\n",
    "\n",
    "zoom_station(df=gdf_stations_w_nation,station=station)\n",
    "location_offset_error(station=station, nation=nation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f040e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now given the nation data we can map stations to railways\n",
    "gdf_stations_w_nation[\"railway\"] = gdf_stations_w_nation.apply(\n",
    "    lambda row: (\n",
    "        \"English Rail\" if row[\"operator\"] == \"Trainline\" and row[\"nation\"] == \"England\" else\n",
    "        \"Scottish Rail\" if row[\"operator\"] == \"Trainline\" and row[\"nation\"] == \"Scotland\" else\n",
    "        \"Welsh Rail\" if row[\"operator\"] == \"Trainline\" and row[\"nation\"] == \"Wales\" else\n",
    "        row[\"operator\"]\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "# Add a flag for trainline figures\n",
    "gdf_stations_w_nation[\"trainline_assisted_sales\"] = (gdf_stations_w_nation[\"operator\"] == \"Trainline\").astype(bool)\n",
    "gdf_stations_w_nation.rename(columns={\"operator\": \"source_operator\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe999b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check one-to-one realationship between stations and operators before engineering steps\n",
    "gdf_stations_w_nation.groupby('station')['source_operator'].nunique().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2a3cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_stations_w_nation.query(\"station == 'Exeter Central'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd1abeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bottom one is correct\n",
    "zoom_station(\n",
    "    df=gdf_stations_w_nation.query(\"station == 'Exeter Central' & source_operator == 'Trainline'\"),\n",
    "    station='Exeter Central'\n",
    "), zoom_station(\n",
    "    df=gdf_stations_w_nation.query(\"station == 'Exeter Central' & source_operator == 'English Rail'\"),\n",
    "    station='Exeter Central'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa4479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because of the incorrect lat/lon we are dropping the row. \n",
    "# Hard to say whether Exeter Central should update to be Trainline or whether the Trainline entry is wrong\n",
    "# As the row was wrong we shall leave Exeter as False for Trainline Assisted Sales\n",
    "gdf_stations_w_nation = gdf_stations_w_nation[~((gdf_stations_w_nation.station == \"Exeter Central\") & (gdf_stations_w_nation.source_operator == \"Trainline\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b546b8",
   "metadata": {},
   "source": [
    "### Join datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b3916b",
   "metadata": {},
   "source": [
    "    It looks like the stations don't match 1-1 between the datasets.\n",
    "\n",
    "    We will need to use the coordinates given to match to downloaded polygons attributing to the area names given to us i.e. Gloucester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d31bb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16af2168",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_authority = gpd.read_file(data_path / \"local_authority.geojson\").to_crs(sales.crs)\n",
    "\n",
    "# Buffer stations by x meters then check point-in-polygon\n",
    "gdf_stations_m = gdf_stations.to_crs(27700)\n",
    "countries_m = countries.to_crs(27700)\n",
    "x = 1000\n",
    "gdf_stations_buf = gdf_stations_m.copy()\n",
    "gdf_stations_buf[\"geometry\"] = gdf_stations_buf.buffer(x)\n",
    "\n",
    "joined = gpd.sjoin(\n",
    "    gdf_stations_buf,\n",
    "    countries_m[[\"CTRY22NM\", \"geometry\"]],\n",
    "    how=\"left\",\n",
    "    predicate=\"intersects\"\n",
    ")\n",
    "gdf_stations_w_nation = (\n",
    "    joined.rename(columns={\"CTRY22NM\": \"nation\"})\n",
    "          .drop(columns=[\"geometry\", \"index_right\"], errors=\"ignore\")\n",
    ")\n",
    "\n",
    "# Checking errors clear\n",
    "gdf_stations_w_nation.query(\"nation.isna()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89e772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_stations_w_nation.query(\"trainline_assisted_sales == True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f812c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.station.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f70dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_stations_w_nation.station.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17927b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_sales = pd.merge(sales, gdf_stations_w_nation[[\"station\", \"nation\", \"railway\", \"trainline_assisted_sales\"]], on=\"station\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfc6503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "992f6dd9",
   "metadata": {},
   "source": [
    "### Adding Date time Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99e46a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_sales[\"date\"] = pd.to_datetime(station_sales[\"date\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "station_sales[\"year\"] = station_sales[\"date\"].dt.year\n",
    "station_sales[\"month\"] = station_sales[\"date\"].dt.month\n",
    "station_sales[\"day\"] = station_sales[\"date\"].dt.day\n",
    "station_sales[\"day_of_week\"] = station_sales[\"date\"].dt.day_name()\n",
    "station_sales[\"quarter\"] = station_sales[\"date\"].dt.quarter\n",
    "station_sales[\"is_weekend\"] = station_sales[\"date\"].dt.dayofweek >= 5\n",
    "\n",
    "uk_holidays = holidays.UnitedKingdom(years=station_sales['date'].dt.year.unique())\n",
    "station_sales['is_holiday'] = station_sales['date'].isin(uk_holidays)\n",
    "station_sales['holiday_name'] = station_sales['date'].map(uk_holidays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff56073",
   "metadata": {},
   "source": [
    "### Number of Sales Adjustments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4549ac",
   "metadata": {},
   "source": [
    "***How do we interpret 'sales'?***\n",
    "\n",
    "    It could be number of ticket sales or the gross revenue. It is hard to say since the value is a decimal.\n",
    "\n",
    "    Spot checking I think it makes more sense to count these as ticket sales\n",
    "\n",
    "***Why?***\n",
    "\n",
    "    11/02/202 | 92.31415573\t| Stratford (London) \n",
    "    is an example of a row.\n",
    "\n",
    "    Stratford is a busy station so the station only making Â£92.31 is unlikely (thats like a few tickets)\n",
    "    Whereas it being the number of tickets is a little more believeable, although trains can have thousands of footfall in one day. I assume for this case study data is at least somewhat synthetic at minimum.\n",
    "\n",
    "***Actions***\n",
    "\n",
    "    You can't have a decimal number of tickets in one day. We will round to the nearest whole number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a465de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rounding\n",
    "station_sales['sales'] = station_sales['sales'].round().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130a2d17",
   "metadata": {},
   "source": [
    "#### Adding lags and rolling features for sales by groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcad903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling_features(\n",
    "    df: pd.DataFrame,\n",
    "    feature: str = \"sales\",\n",
    "    group_col: str = \"station\",\n",
    "    date_col: str = \"date\",\n",
    "    windows: list | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    if windows is None:\n",
    "        windows = [1, 3, 7, 14, 30, 60, 90, 180]\n",
    "\n",
    "    out = df.copy()\n",
    "    out.sort_values([group_col, date_col], inplace=True)\n",
    "\n",
    "    cols = []\n",
    "    for w in windows:\n",
    "        cols += [\n",
    "            f\"{group_col}_by_{feature}_mean_{w}d\",\n",
    "            f\"{group_col}_by_{feature}_std_{w}d\",\n",
    "            f\"{group_col}_by_{feature}_sum_{w}d\",\n",
    "        ]\n",
    "    for c in cols:\n",
    "        out[c] = np.nan\n",
    "\n",
    "    for _, g in out.groupby(group_col, sort=False):\n",
    "        s = g.set_index(date_col)[feature]\n",
    "\n",
    "        for w in windows:\n",
    "            win = f\"{w}D\"\n",
    "            mean_vals = s.rolling(win, min_periods=1).mean().to_numpy()\n",
    "            std_vals  = s.rolling(win, min_periods=1).std().to_numpy()\n",
    "            sum_vals  = s.rolling(win, min_periods=1).sum().to_numpy()\n",
    "\n",
    "            out.loc[g.index, f\"{group_col}_by_{feature}_mean_{w}d\"] = mean_vals\n",
    "            out.loc[g.index, f\"{group_col}_by_{feature}_std_{w}d\"]  = std_vals\n",
    "            out.loc[g.index, f\"{group_col}_by_{feature}_sum_{w}d\"]  = sum_vals\n",
    "\n",
    "    return out\n",
    "\n",
    "def add_lag_features(\n",
    "    df: pd.DataFrame,\n",
    "    feature: str = \"sales\",\n",
    "    group_col: str = \"station\",\n",
    "    date_col: str = \"date\",\n",
    "    windows: list | None = None,\n",
    "    daily_agg: str = \"sum\",\n",
    ") -> pd.DataFrame:\n",
    "    if windows is None:\n",
    "        windows = [1, 3, 7, 14, 30, 60, 90, 180]\n",
    "\n",
    "    out = df.copy()\n",
    "    out[date_col] = pd.to_datetime(out[date_col], errors=\"coerce\")\n",
    "    out.sort_values([group_col, date_col], inplace=True)\n",
    "\n",
    "    for w in windows:\n",
    "        out[f\"{group_col}_by_{feature}_lag_{w}d\"] = np.nan\n",
    "\n",
    "    # compute per group\n",
    "    for _, g in out.groupby(group_col, sort=False):\n",
    "        daily = (\n",
    "            g[[date_col, feature]]\n",
    "            .groupby(date_col, as_index=True)\n",
    "            .agg({feature: daily_agg})\n",
    "            .sort_index()\n",
    "        )\n",
    "\n",
    "        for w in windows:\n",
    "            lag_daily = daily[feature].shift(freq=f\"{w}D\")\n",
    "            vals = lag_daily.reindex(g[date_col]).to_numpy()\n",
    "            out.loc[g.index, f\"{group_col}_by_{feature}_lag_{w}d\"] = vals\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c07a7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_sales = (\n",
    "    station_sales\n",
    "      .pipe(add_rolling_features, group_col=\"station\", feature=\"sales\")\n",
    "      .pipe(add_rolling_features, group_col=\"railway\", feature=\"sales\")\n",
    "      .pipe(add_rolling_features, group_col=\"nation\",  feature=\"sales\")\n",
    "      .pipe(add_lag_features, group_col=\"station\", feature=\"sales\")\n",
    "      .pipe(add_lag_features, group_col=\"railway\", feature=\"sales\")\n",
    "      .pipe(add_lag_features, group_col=\"nation\",  feature=\"sales\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1f82bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_sales.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d353a24",
   "metadata": {},
   "source": [
    "### Trainline Features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e34dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = station_sales.copy()\n",
    "\n",
    "# 1) Normalize to strict bool, treating NaN as False\n",
    "df['tl_flag'] = (\n",
    "    df['trainline_assisted_sales']\n",
    "      .fillna(False)\n",
    "      .astype(bool)\n",
    ")\n",
    "\n",
    "# 2) Enforce station-level consistency (take any True observed for a station)\n",
    "station_tl = (df.groupby('station', as_index=False)['tl_flag']\n",
    "                .any()\n",
    "                .rename(columns={'tl_flag':'station_tl_flag'}))\n",
    "\n",
    "# df = df.drop(columns='tl_flag').merge(station_tl, on='station', how='left')\n",
    "# df['station_tl_flag'] = df['station_tl_flag'].fillna(False)\n",
    "\n",
    "# # 3) Compute TL sales: sales * flag\n",
    "# df['trainline_sales'] = df['sales'] * df['station_tl_flag'].astype(int)\n",
    "\n",
    "# # 4) Aggregate per date + nation\n",
    "# agg_nation = (df.groupby(['date','nation'], as_index=False)\n",
    "#                 .agg(total_sales=('sales','sum'),\n",
    "#                      trainline_sales=('trainline_sales','sum')))\n",
    "# agg_nation['trainline_share'] = agg_nation['trainline_sales'] / agg_nation['total_sales']\n",
    "\n",
    "# # 5) Aggregate per date + railway (totals are for that day only because 'date' is in the groupby)\n",
    "# agg_railway = (df.groupby(['date','railway'], as_index=False)\n",
    "#                 .agg(total_sales=('sales','sum'),\n",
    "#                      trainline_sales=('trainline_sales','sum')))\n",
    "# agg_railway['trainline_share'] = agg_railway['trainline_sales'] / agg_railway['total_sales']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70ecf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['trainline_assisted_sales'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216f5c37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "case-study-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
